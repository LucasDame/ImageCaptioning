# Guide des Mod√®les - Image Captioning

## üìö Vue d'ensemble

Ce guide explique l'architecture des mod√®les pour le projet d'image captioning.

## üèóÔ∏è Architecture Globale

```
Image (224x224x3)
        ‚Üì
   ENCODER CNN
        ‚Üì
Feature Vector (512)
        ‚Üì
   DECODER LSTM (avec embeddings)
        ‚Üì
Caption (s√©quence de mots)
```

## üîç 1. Encoder CNN (`encoder.py`)

### R√¥le
Extraire les features visuelles d'une image et les transformer en un vecteur de taille fixe.

### Deux versions disponibles

#### EncoderCNN (Version compl√®te)
- **Architecture**: 5 blocs convolutionnels
- **Param√®tres**: ~15M
- **Meilleure qualit√©** mais plus lent √† entra√Æner

```python
from models import EncoderCNN

encoder = EncoderCNN(feature_dim=512)
```

**Architecture d√©taill√©e**:
```
Input: (batch_size, 3, 224, 224)
  ‚Üì
Bloc 1: Conv(3‚Üí64) + BN + ReLU + MaxPool ‚Üí (64, 112, 112)
  ‚Üì
Bloc 2: Conv(64‚Üí128) + BN + ReLU + MaxPool ‚Üí (128, 56, 56)
  ‚Üì
Bloc 3: 2√óConv(128‚Üí256) + BN + ReLU + MaxPool ‚Üí (256, 28, 28)
  ‚Üì
Bloc 4: 2√óConv(256‚Üí512) + BN + ReLU + MaxPool ‚Üí (512, 14, 14)
  ‚Üì
Bloc 5: 2√óConv(512‚Üí512) + BN + ReLU + MaxPool ‚Üí (512, 7, 7)
  ‚Üì
AdaptiveAvgPool ‚Üí (512, 1, 1)
  ‚Üì
Flatten + FC(512‚Üí512) + ReLU + Dropout
  ‚Üì
Output: (batch_size, 512)
```

#### EncoderCNNLite (Version l√©g√®re)
- **Architecture**: 4 blocs convolutionnels
- **Param√®tres**: ~2M
- **Plus rapide** pour tester et d√©velopper

```python
from models import EncoderCNNLite

encoder = EncoderCNNLite(feature_dim=512)
```

**Architecture d√©taill√©e**:
```
Input: (batch_size, 3, 224, 224)
  ‚Üì
Bloc 1: Conv(3‚Üí32) + BN + ReLU + MaxPool ‚Üí (32, 56, 56)
  ‚Üì
Bloc 2: Conv(32‚Üí64) + BN + ReLU + MaxPool ‚Üí (64, 28, 28)
  ‚Üì
Bloc 3: Conv(64‚Üí128) + BN + ReLU + MaxPool ‚Üí (128, 14, 14)
  ‚Üì
Bloc 4: Conv(128‚Üí256) + BN + ReLU + MaxPool ‚Üí (256, 7, 7)
  ‚Üì
AdaptiveAvgPool ‚Üí (256, 1, 1)
  ‚Üì
Flatten + FC(256‚Üí512) + ReLU + Dropout
  ‚Üì
Output: (batch_size, 512)
```

### Composants cl√©s

**Convolution (Conv2D)**:
- Extrait des features locales (bords, textures, formes)
- kernel_size=3: Petite fen√™tre pour capturer les d√©tails

**Batch Normalization (BN)**:
- Normalise les activations
- Acc√©l√®re l'entra√Ænement et am√©liore la stabilit√©

**ReLU**:
- Fonction d'activation: `f(x) = max(0, x)`
- Introduit la non-lin√©arit√©

**MaxPooling**:
- R√©duit la dimension spatiale
- Garde les features les plus importantes

**AdaptiveAvgPool**:
- Force une taille de sortie fixe (1√ó1)
- Permet de g√©rer diff√©rentes tailles d'input

**Dropout**:
- R√©gularisation pour √©viter l'overfitting
- D√©sactive al√©atoirement 50% des neurones pendant l'entra√Ænement

### Exemple d'utilisation

```python
import torch
from models import EncoderCNN

# Cr√©er l'encoder
encoder = EncoderCNN(feature_dim=512)

# Image batch
images = torch.randn(4, 3, 224, 224)  # 4 images

# Forward pass
features = encoder(images)  # (4, 512)

print(f"Features shape: {features.shape}")
print(f"Nombre de param√®tres: {encoder.get_num_params():,}")
```

## üß† 2. Decoder LSTM (`decoder.py`)

### R√¥le
G√©n√©rer une caption mot par mot √† partir des features de l'image.

### Architecture

```python
from models import DecoderLSTM

decoder = DecoderLSTM(
    feature_dim=512,      # Dimension des features de l'encoder
    embedding_dim=256,    # Dimension des word embeddings
    hidden_dim=512,       # Dimension du LSTM
    vocab_size=5000,      # Taille du vocabulaire
    num_layers=1,         # Nombre de couches LSTM
    dropout=0.5           # Taux de dropout
)
```

### Composants cl√©s

#### 1. Embedding Layer
Convertit les indices de mots en vecteurs denses.

```python
# Vocabulaire
word2idx = {
    '<PAD>': 0,
    '<START>': 1,
    'dog': 45,
    'running': 123,
    '<END>': 2
}

# Embedding
embedding = nn.Embedding(
    num_embeddings=5000,  # Taille du vocabulaire
    embedding_dim=256,    # Dimension du vecteur
    padding_idx=0         # Index de <PAD>
)

# Utilisation
word_indices = torch.tensor([1, 45, 123, 2])  # [START, dog, running, END]
embedded = embedding(word_indices)  # (4, 256)
```

**Chaque mot devient un vecteur de 256 dimensions** que le LSTM peut traiter.

#### 2. Feature Projection
Projette les features de l'image dans l'espace du LSTM.

```python
feature_projection = nn.Linear(512, 512)  # feature_dim ‚Üí hidden_dim
```

Les features deviennent le **hidden state initial** du LSTM.

#### 3. LSTM
Le c≈ìur du decoder. G√©n√®re la s√©quence de mots.

```python
lstm = nn.LSTM(
    input_size=256,      # embedding_dim
    hidden_size=512,     # hidden_dim
    num_layers=1,
    batch_first=True
)
```

**Comment fonctionne le LSTM ?**

```
t=0: [START] ‚Üí LSTM ‚Üí pr√©dit "a"
      ‚Üë
  image features (hidden state initial)

t=1: "a" ‚Üí LSTM ‚Üí pr√©dit "dog"
      ‚Üë
  hidden state de t=0

t=2: "dog" ‚Üí LSTM ‚Üí pr√©dit "is"
      ‚Üë
  hidden state de t=1

t=3: "is" ‚Üí LSTM ‚Üí pr√©dit "running"
...
```

Le LSTM garde une **m√©moire** des mots pr√©c√©dents gr√¢ce √† son hidden state.

#### 4. Output Layer
Projette le hidden state du LSTM vers le vocabulaire.

```python
fc = nn.Linear(hidden_dim, vocab_size)  # 512 ‚Üí 5000
```

Pour chaque position, on obtient un **score pour chaque mot** du vocabulaire.

### Flow complet du Decoder

```
Image Features (512)
        ‚Üì
Feature Projection ‚Üí Hidden State Initial (512)
        ‚Üì
Word "a" (index 45)
        ‚Üì
Embedding Layer ‚Üí Vector (256)
        ‚Üì
LSTM (avec hidden state) ‚Üí Output (512)
        ‚Üì
FC Layer ‚Üí Scores pour tous les mots (5000)
        ‚Üì
Softmax ‚Üí Probabilit√©s
        ‚Üì
Argmax ‚Üí Mot pr√©dit
```

### Teacher Forcing (Entra√Ænement)

Pendant l'entra√Ænement, on utilise **les vrais mots pr√©c√©dents**, pas les pr√©dictions.

```python
# Caption: "a dog is running"
# Indices: [1, 45, 123, 67, 89, 2]
#          [START, a, dog, is, running, END]

# Input au decoder: [1, 45, 123, 67, 89]  (sans END)
# Target:           [45, 123, 67, 89, 2]  (sans START)

outputs = decoder(features, inputs)
loss = criterion(outputs, targets)
```

**Pourquoi ?** C'est plus stable et plus rapide que d'utiliser les pr√©dictions.

### G√©n√©ration (Inf√©rence)

En inf√©rence, on g√©n√®re **mot par mot** en utilisant les pr√©dictions pr√©c√©dentes.

```python
# M√©thode greedy (simple)
caption = decoder.generate(
    features,
    max_length=20,
    start_token=1,
    end_token=2
)
```

**Algorithme**:
1. Commencer avec `<START>`
2. Pr√©dire le mot le plus probable
3. Utiliser ce mot comme input pour l'√©tape suivante
4. R√©p√©ter jusqu'√† `<END>` ou max_length

### Exemple d'utilisation

```python
import torch
from models import DecoderLSTM

# Cr√©er le decoder
decoder = DecoderLSTM(
    feature_dim=512,
    embedding_dim=256,
    hidden_dim=512,
    vocab_size=5000,
    num_layers=1,
    dropout=0.5
)

# Features de l'encoder
features = torch.randn(4, 512)  # 4 images

# Captions (indices)
captions = torch.randint(0, 5000, (4, 15))  # 4 captions de longueur 15

# Forward pass (entra√Ænement)
outputs = decoder(features, captions)  # (4, 15, 5000)

# G√©n√©ration (inf√©rence)
generated = decoder.generate(features[:1], max_length=20)  # (1, seq_len)
```

## üéØ 3. Mod√®le Complet (`caption_model.py`)

### ImageCaptioningModel

Combine l'encoder et le decoder.

```python
from models import create_model

model = create_model(
    vocab_size=5000,
    embedding_dim=256,
    hidden_dim=512,
    feature_dim=512,
    num_layers=1,
    dropout=0.5,
    encoder_type='lite'  # 'full' ou 'lite'
)
```

### Forward Pass (Entra√Ænement)

```python
# Batch d'images et captions
images = torch.randn(4, 3, 224, 224)
captions = torch.randint(0, 5000, (4, 15))

# Forward
outputs = model(images, captions)  # (4, 15, 5000)

# Loss
inputs = captions[:, :-1]
targets = captions[:, 1:]
outputs_reshaped = outputs.reshape(-1, vocab_size)
targets_reshaped = targets.reshape(-1)
loss = criterion(outputs_reshaped, targets_reshaped)
```

### G√©n√©ration de Caption

```python
# Image unique
image = torch.randn(1, 3, 224, 224)

# G√©n√©rer
caption_indices = model.generate_caption(
    image,
    max_length=20,
    start_token=1,
    end_token=2,
    method='greedy'
)

# Convertir en texte
caption_text = vocabulary.denumericalize(caption_indices[0])
print(caption_text)  # "a dog is running in the park"
```

### Sauvegarde et Chargement

```python
from models import save_model, load_model

# Sauvegarder
save_model(
    model,
    'checkpoints/best_model.pth',
    optimizer=optimizer,
    epoch=10,
    loss=1.5,
    vocab=vocabulary
)

# Charger
model, info = load_model(
    'checkpoints/best_model.pth',
    device='cuda',
    encoder_type='lite'
)
```

## üìä 4. Choix des Hyperparam√®tres

### Dimensions

| Hyperparam√®tre | Petit | Moyen | Grand |
|----------------|-------|-------|-------|
| embedding_dim  | 128   | 256   | 512   |
| hidden_dim     | 256   | 512   | 1024  |
| feature_dim    | 256   | 512   | 1024  |
| num_layers     | 1     | 2     | 3     |

**Recommand√© pour Flickr8k**: `embedding_dim=256`, `hidden_dim=512`, `feature_dim=512`, `num_layers=1`

### R√©gularisation

- **dropout**: 0.3-0.5 (0.5 recommand√©)
- **weight_decay**: 1e-5 (dans l'optimizer)

### Trade-offs

**Plus de param√®tres**:
- ‚úÖ Meilleure capacit√© du mod√®le
- ‚úÖ Peut apprendre des patterns complexes
- ‚ùå Risque d'overfitting
- ‚ùå Plus lent √† entra√Æner
- ‚ùå Plus de m√©moire GPU

**Moins de param√®tres**:
- ‚úÖ Plus rapide
- ‚úÖ Moins de m√©moire
- ‚úÖ Moins d'overfitting
- ‚ùå Capacit√© limit√©e

## üî¨ 5. Tests des Mod√®les

### Test de l'Encoder

```bash
python models/encoder.py
```

Sortie attendue:
```
Features output shape: torch.Size([4, 512])
Number of parameters: 2,XXX,XXX
```

### Test du Decoder

```bash
python models/decoder.py
```

Sortie attendue:
```
Outputs shape: torch.Size([4, 15, 5000])
Generated caption shape: torch.Size([1, seq_len])
```

### Test du Mod√®le Complet

```bash
python models/caption_model.py
```

## üìà 6. Complexit√© Computationnelle

### EncoderCNN (Lite)
- **Param√®tres**: ~2M
- **FLOPs**: ~0.5G par image
- **M√©moire GPU**: ~500MB (batch_size=32)

### EncoderCNN (Full)
- **Param√®tres**: ~15M
- **FLOPs**: ~3G par image
- **M√©moire GPU**: ~2GB (batch_size=32)

### DecoderLSTM
- **Param√®tres**: ~5-10M (d√©pend de vocab_size)
- **M√©moire GPU**: ~1GB (batch_size=32)

### Total (Lite)
- **Param√®tres**: ~7-12M
- **M√©moire GPU**: ~1.5-2GB (batch_size=32)
- **Temps d'entra√Ænement**: ~2-3h pour 30 epochs (GPU moderne)

## ‚ùì FAQ

**Q: Quelle version de l'encoder utiliser ?**
A: `EncoderCNNLite` pour d√©velopper et tester rapidement. `EncoderCNN` pour les meilleurs r√©sultats finaux.

**Q: Pourquoi utiliser LSTM au lieu de Transformer ?**
A: Les LSTM sont plus simples √† impl√©menter from scratch et fonctionnent bien pour les s√©quences courtes (captions).

**Q: Combien de couches LSTM utiliser ?**
A: 1 couche suffit g√©n√©ralement. 2-3 couches peuvent am√©liorer l√©g√®rement mais augmentent l'overfitting.

**Q: Que faire si j'ai une erreur "out of memory" ?**
A: R√©duire `batch_size`, utiliser `EncoderCNNLite`, ou r√©duire `hidden_dim`.

**Q: Les embeddings doivent-ils √™tre pr√©-entra√Æn√©s ?**
A: Non ! Le projet est from scratch. Les embeddings s'entra√Ænent en m√™me temps que le reste.

## üîó Ressources

- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Show and Tell Paper](https://arxiv.org/abs/1411.4555)
- [PyTorch LSTM Tutorial](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)