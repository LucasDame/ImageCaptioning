# ğŸš€ Guide de DÃ©marrage Rapide

## Installation et Premier EntraÃ®nement

### 1. Installation des dÃ©pendances

```bash
pip install -r requirements.txt
```

### 2. TÃ©lÃ©charger le dataset Flickr8k

Structure attendue:
```
data/
â””â”€â”€ flickr8k/
    â”œâ”€â”€ Images/           # Toutes les images
    â””â”€â”€ captions.txt      # Fichier de captions
```

Format de `captions.txt`:
```
image1.jpg	a dog running in the park
image1.jpg	a brown dog is running
image2.jpg	two cats sitting on a wall
...
```

### 3. PrÃ©parer les donnÃ©es (optionnel mais recommandÃ©)

```bash
python prepare_data.py
```

Ce script va:
- Charger les captions
- Construire le vocabulaire
- CrÃ©er les splits train/val/test
- Tester un batch
- Sauvegarder le vocabulaire dans `data/vocab.pkl`

### 4. EntraÃ®ner le modÃ¨le

```bash
python train.py
```

**Configuration par dÃ©faut** (dans `train.py`):
- Encoder: `lite` (rapide)
- Epochs: 30
- Batch size: 32
- Learning rate: 0.001
- Embedding dim: 256
- Hidden dim: 512

**RÃ©sultats**:
- ModÃ¨les sauvegardÃ©s dans `checkpoints/`
- Meilleur modÃ¨le: `checkpoints/best_model.pth`
- Courbes d'apprentissage: `logs/learning_curves.png`
- Historique: `logs/training_history.json`

### 5. Ã‰valuer le modÃ¨le

```bash
python evaluate.py
```

Calcule les mÃ©triques BLEU sur le test set et gÃ©nÃ¨re:
- `results/evaluation_results.json` (scores BLEU)
- `results/caption_examples.json` (exemples de captions)

### 6. Tester avec une image (DÃ©mo)

```bash
# Image unique
python demo.py --image data/test_image.jpg

# Avec sauvegarde du rÃ©sultat
python demo.py --image data/test_image.jpg --output results/demo.png

# Batch d'images
python demo.py --image data/test_images/ --batch --output results/batch/
```

---

## ğŸ“ Structure du Projet

```
image-captioning/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ flickr8k/
â”‚   â”‚   â”œâ”€â”€ Images/              # Images du dataset
â”‚   â”‚   â””â”€â”€ captions.txt         # Fichier de captions
â”‚   â””â”€â”€ vocab.pkl                # Vocabulaire (gÃ©nÃ©rÃ©)
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ vocabulary.py            # Gestion du vocabulaire
â”‚   â”œâ”€â”€ preprocessing.py         # Preprocessing images/captions
â”‚   â””â”€â”€ data_loader.py           # DataLoader PyTorch
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ encoder.py               # CNN encoder
â”‚   â”œâ”€â”€ decoder.py               # LSTM decoder
â”‚   â””â”€â”€ caption_model.py         # ModÃ¨le complet
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ EMBEDDING_GUIDE.py       # Guide sur les embeddings
â”‚   â”œâ”€â”€ PREPROCESSING_GUIDE.md   # Guide du preprocessing
â”‚   â””â”€â”€ MODELS_GUIDE.md          # Guide des modÃ¨les
â”‚
â”œâ”€â”€ checkpoints/                 # ModÃ¨les sauvegardÃ©s (gÃ©nÃ©rÃ©)
â”œâ”€â”€ logs/                        # Logs d'entraÃ®nement (gÃ©nÃ©rÃ©)
â”œâ”€â”€ results/                     # RÃ©sultats d'Ã©valuation (gÃ©nÃ©rÃ©)
â”‚
â”œâ”€â”€ prepare_data.py              # Script de prÃ©paration
â”œâ”€â”€ train.py                     # Script d'entraÃ®nement
â”œâ”€â”€ evaluate.py                  # Script d'Ã©valuation
â”œâ”€â”€ demo.py                      # Script de dÃ©mo
â”œâ”€â”€ requirements.txt             # DÃ©pendances
â””â”€â”€ README.md                    # Documentation principale
```

---

## ğŸ¯ Workflow Typique

### Phase 1: DÃ©veloppement

1. **Tester le preprocessing**
   ```bash
   python prepare_data.py
   ```

2. **EntraÃ®nement rapide** (pour tester)
   ```python
   # Modifier train.py:
   config = {
       'num_epochs': 5,          # Juste 5 epochs
       'encoder_type': 'lite',   # Encoder lÃ©ger
       'batch_size': 64,         # Plus grand batch
   }
   ```

3. **VÃ©rifier que tout fonctionne**
   ```bash
   python train.py
   python evaluate.py
   python demo.py --image test.jpg
   ```

### Phase 2: EntraÃ®nement Final

1. **Configuration optimale**
   ```python
   config = {
       'num_epochs': 30-50,      # Plus d'epochs
       'encoder_type': 'lite',   # Ou 'full' si GPU puissant
       'batch_size': 32,
       'learning_rate': 0.001,
   }
   ```

2. **Lancer l'entraÃ®nement**
   ```bash
   python train.py
   ```
   
   â±ï¸ Temps estimÃ©:
   - Lite encoder: ~2-3h (GPU moderne)
   - Full encoder: ~6-8h (GPU moderne)

3. **Surveiller les courbes**
   - Regarder `logs/learning_curves.png` aprÃ¨s chaque epoch
   - Train loss et val loss doivent descendre
   - Si val loss augmente â†’ overfitting

### Phase 3: Ã‰valuation et DÃ©mo

1. **Ã‰valuer sur le test set**
   ```bash
   python evaluate.py
   ```
   
   Scores BLEU attendus (Flickr8k, from scratch):
   - BLEU-1: 0.50-0.60
   - BLEU-2: 0.30-0.40
   - BLEU-3: 0.20-0.25
   - BLEU-4: 0.15-0.20

2. **PrÃ©parer la dÃ©mo pour l'examen**
   ```bash
   # Tester avec l'image fournie
   python demo.py --image exam_image.jpg --output results/exam_demo.png
   ```

---

## ğŸ› ï¸ Modifier la Configuration

### Changer les hyperparamÃ¨tres

Ã‰diter `train.py`:

```python
config = {
    # ModÃ¨le
    'embedding_dim': 256,      # â†‘ pour plus de capacitÃ©
    'hidden_dim': 512,         # â†‘ pour plus de capacitÃ©
    'num_layers': 1,           # â†‘ pour LSTM plus profond
    'dropout': 0.5,            # â†‘ si overfitting
    
    # EntraÃ®nement
    'num_epochs': 30,          # â†‘ pour converger
    'batch_size': 32,          # â†“ si out of memory
    'learning_rate': 0.001,    # â†“ si loss instable
    
    # Encoder
    'encoder_type': 'lite',    # 'full' pour meilleure qualitÃ©
}
```

### Utiliser l'encoder complet

```python
config['encoder_type'] = 'full'
```

âš ï¸ NÃ©cessite plus de mÃ©moire GPU

### Changer la taille des images

```python
config['image_size'] = 224  # Ou 128 pour plus rapide
```

---

## ğŸ› RÃ©solution de ProblÃ¨mes

### Out of Memory (GPU)

**Solutions**:
1. RÃ©duire `batch_size`: 32 â†’ 16 â†’ 8
2. Utiliser `encoder_type='lite'`
3. RÃ©duire `image_size`: 224 â†’ 128
4. RÃ©duire `hidden_dim`: 512 â†’ 256

### Loss ne descend pas

**VÃ©rifications**:
1. Le vocabulaire est-il construit ? (`data/vocab.pkl` existe ?)
2. Les donnÃ©es sont-elles chargÃ©es ? (pas d'erreur dans `prepare_data.py` ?)
3. Le learning rate est-il trop bas ? (essayer 0.001)
4. Y a-t-il assez de donnÃ©es ? (Flickr8k = 8000 images)

### Loss explose (NaN)

**Solutions**:
1. RÃ©duire le learning rate: 0.001 â†’ 0.0001
2. VÃ©rifier que gradient clipping est activÃ© (dÃ©jÃ  dans `train.py`)
3. VÃ©rifier les donnÃ©es (pas de valeurs NaN)

### Captions gÃ©nÃ©rÃ©es sont bizarres

**Normal si**:
- DÃ©but d'entraÃ®nement (< 5 epochs)
- ModÃ¨le pas assez entraÃ®nÃ©
- DonnÃ©es insuffisantes

**Solutions**:
- EntraÃ®ner plus longtemps (30+ epochs)
- VÃ©rifier que val loss descend
- Augmenter la capacitÃ© du modÃ¨le

---

## ğŸ“Š InterprÃ©ter les RÃ©sultats

### Courbes d'apprentissage

```
Loss
 â”‚
 â”‚ â•²
 â”‚  â•²  Train Loss
 â”‚   â•²___________
 â”‚
 â”‚    â•²
 â”‚     â•²  Val Loss
 â”‚      â•²_________
 â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epochs
```

**Bon signe**: Les deux descendent et convergent

**Overfitting**: Train loss descend mais val loss augmente
â†’ Augmenter dropout, rÃ©duire num_layers

**Underfitting**: Les deux sont hautes et stables
â†’ Augmenter capacitÃ© du modÃ¨le, entraÃ®ner plus

### Scores BLEU

- **BLEU-1**: Compte les mots individuels (facile)
- **BLEU-2**: Compte les paires de mots (plus dur)
- **BLEU-3**: Compte les triplets (encore plus dur)
- **BLEU-4**: Compte les 4-grammes (le plus dur)

**InterprÃ©tation**:
- BLEU-4 > 0.20 â†’ Excellent (rare from scratch)
- BLEU-4 > 0.15 â†’ TrÃ¨s bon
- BLEU-4 > 0.10 â†’ Bon
- BLEU-4 < 0.10 â†’ Ã€ amÃ©liorer

---

## ğŸ“ Pour l'Examen Final

### Checklist de prÃ©paration

- [ ] ModÃ¨le entraÃ®nÃ© (30+ epochs)
- [ ] Meilleur checkpoint sauvegardÃ©
- [ ] Script de dÃ©mo testÃ©
- [ ] Image de test prÃªte
- [ ] Comprendre l'architecture
- [ ] Pouvoir expliquer les choix

### Questions Potentielles

**Architecture**:
- Pourquoi CNN pour l'encoder ?
- Pourquoi LSTM pour le decoder ?
- Qu'est-ce que le teacher forcing ?
- Comment fonctionnent les embeddings ?

**EntraÃ®nement**:
- Quelle loss avez-vous utilisÃ©e ?
- Quels sont vos hyperparamÃ¨tres ?
- Comment gÃ©rez-vous le padding ?
- Combien de temps pour entraÃ®ner ?

**RÃ©sultats**:
- Quels sont vos scores BLEU ?
- Montrer des exemples de succÃ¨s/Ã©checs
- Limites du modÃ¨le ?
- Comment amÃ©liorer ?

### DÃ©mo Live

```bash
# Script simple pour la dÃ©mo
python demo.py --image <image_fournie.jpg> --output results/demo_final.png
```

**Timing**: < 5 secondes pour gÃ©nÃ©rer une caption

---

## ğŸ’¡ Conseils

### Pour gagner du temps

1. **Utilisez `EncoderCNNLite`** pendant le dÃ©veloppement
2. **Testez sur un subset** des donnÃ©es d'abord
3. **Sauvegardez rÃ©guliÃ¨rement** les checkpoints
4. **Utilisez un GPU** si disponible

### Pour de meilleurs rÃ©sultats

1. **EntraÃ®nez plus longtemps** (30-50 epochs)
2. **Augmentez la capacitÃ©** (hidden_dim, embedding_dim)
3. **Essayez diffÃ©rents learning rates**
4. **Ajustez le dropout** selon l'overfitting

### Pour la prÃ©sentation

1. **PrÃ©parez plusieurs exemples** (bons et moins bons)
2. **Expliquez vos choix** (pourquoi ces hyperparamÃ¨tres ?)
3. **Montrez les courbes** d'apprentissage
4. **Soyez honnÃªte** sur les limites

---

## ğŸ”— Documentation ComplÃ¨te

- **README.md**: Vue d'ensemble du projet
- **PREPROCESSING_GUIDE.md**: Guide du preprocessing
- **EMBEDDING_GUIDE.py**: Guide des embeddings
- **MODELS_GUIDE.md**: Guide des modÃ¨les

Bon courage ! ğŸ“