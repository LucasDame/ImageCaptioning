# Image Captioning from Scratch

## ğŸ“‹ Description du Projet

Projet de Deep Learning consistant Ã  dÃ©velopper un systÃ¨me de gÃ©nÃ©ration automatique de lÃ©gendes pour des images, **entiÃ¨rement from scratch avec PyTorch** (sans modÃ¨les prÃ©-entraÃ®nÃ©s).

Le modÃ¨le utilise une architecture encoder-decoder :
- **Encoder (CNN)** : Extrait les caractÃ©ristiques visuelles de l'image
- **Decoder (LSTM)** : GÃ©nÃ¨re la lÃ©gende mot par mot Ã  partir des features

## ğŸ¯ Objectifs

- ImplÃ©menter une architecture complÃ¨te d'image captioning sans utiliser de modÃ¨les prÃ©-entraÃ®nÃ©s
- EntraÃ®ner le modÃ¨le sur un dataset standard (COCO, Flickr8k ou Flickr30k)
- PrÃ©parer une dÃ©mo live pour la session d'examen finale
- Travailler en Ã©quipe de deux personnes

## ğŸ—‚ï¸ Structure du Projet

```
image-captioning/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # DonnÃ©es brutes
â”‚   â”œâ”€â”€ processed/              # DonnÃ©es prÃ©traitÃ©es
â”‚   â””â”€â”€ vocab.pkl              # Vocabulaire construit
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ encoder.py             # Architecture CNN
â”‚   â”œâ”€â”€ decoder.py             # Architecture LSTM
â”‚   â””â”€â”€ caption_model.py       # ModÃ¨le complet
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_loader.py         # Chargement des donnÃ©es
â”‚   â”œâ”€â”€ vocabulary.py          # Construction du vocabulaire
â”‚   â””â”€â”€ preprocessing.py       # PrÃ©traitement des images
â”œâ”€â”€ train.py                   # Script d'entraÃ®nement
â”œâ”€â”€ evaluate.py                # Script d'Ã©valuation
â”œâ”€â”€ demo.py                    # Script pour la dÃ©mo
â”œâ”€â”€ requirements.txt           # DÃ©pendances
â””â”€â”€ README.md                  # Ce fichier
```

## ğŸ› ï¸ Technologies UtilisÃ©es

- **Python 3.9** : Langage de programmation
- **PyTorch** : Framework de deep learning
- **torchvision** : Manipulation d'images
- **NumPy** : Calculs numÃ©riques
- **Pillow** : Traitement d'images
- **Matplotlib** : Visualisation
- **NLTK** : Traitement du langage naturel

## ğŸ“Š Datasets Possibles

1. **Flickr8k** (recommandÃ© pour dÃ©buter)
   - 8,000 images
   - 5 captions par image
   - Plus lÃ©ger, entraÃ®nement plus rapide

2. **COCO**
   - 120,000+ images
   - 5 captions par image
   - Plus complexe, meilleurs rÃ©sultats

## ğŸ—ºï¸ Feuille de Route

### Phase 1 : PrÃ©paration et ComprÃ©hension

#### âœ… TÃ¢ches Ã  rÃ©aliser
- [x] Lire et comprendre l'architecture encoder-decoder
- [x] Ã‰tudier le fonctionnement des CNN et LSTM
- [x] Choisir le dataset 
- [x] TÃ©lÃ©charger le dataset
- [x] Configurer l'environnement de dÃ©veloppement
- [x] Installer les dÃ©pendances

---

### Phase 2 : PrÃ©traitement des DonnÃ©es

#### âœ… TÃ¢ches Ã  rÃ©aliser
- [x] ImplÃ©menter le chargement des images
- [x] CrÃ©er la classe `Vocabulary` pour construire le vocabulaire
- [x] Tokenizer les captions (ajout des tokens `<start>`, `<end>`, `<pad>`, `<unk>`)
- [x] Normaliser les images (resize, normalisation)
- [x] CrÃ©er le `DataLoader` PyTorch
- [x] Diviser les donnÃ©es (train/val/test)

#### ğŸ“ DÃ©tails techniques
```python
# Tokens spÃ©ciaux
<start> : DÃ©but de sÃ©quence
<end>   : Fin de sÃ©quence
<pad>   : Padding
<unk>   : Mots inconnus
```

---

### Phase 3 : ImplÃ©mentation de l'Encoder 

#### âœ… TÃ¢ches Ã  rÃ©aliser
- [ ] Concevoir l'architecture CNN from scratch
- [ ] ImplÃ©menter les couches convolutionnelles
- [ ] Ajouter le pooling et la normalisation
- [ ] CrÃ©er la couche fully connected pour extraire le feature vector
- [ ] Tester l'encoder sur quelques images

#### ğŸ—ï¸ Architecture suggÃ©rÃ©e
```
Input (224x224x3)
â†’ Conv2D(64) + ReLU + MaxPool
â†’ Conv2D(128) + ReLU + MaxPool
â†’ Conv2D(256) + ReLU + MaxPool
â†’ Conv2D(512) + ReLU + MaxPool
â†’ Flatten
â†’ Linear(2048) â†’ Feature vector
```

---

### Phase 4 : ImplÃ©mentation du Decoder 

#### âœ… TÃ¢ches Ã  rÃ©aliser
- [ ] ImplÃ©menter la couche d'embedding pour les mots
- [ ] CrÃ©er l'architecture LSTM
- [ ] ImplÃ©menter la couche de sortie (softmax)
- [ ] GÃ©rer les sÃ©quences de longueur variable
- [ ] Tester le decoder avec des features alÃ©atoires

#### ğŸ—ï¸ Architecture suggÃ©rÃ©e
```
Feature vector (2048)
â†’ Linear projection
â†’ Embedding layer pour les mots (W_emb)
â†’ LSTM cells (sÃ©quence)
â†’ Linear â†’ Softmax (prÃ©diction du prochain mot)
```

---

### Phase 5 : Assemblage du ModÃ¨le Complet

#### âœ… TÃ¢ches Ã  rÃ©aliser
- [ ] Combiner encoder et decoder
- [ ] ImplÃ©menter la forward pass complÃ¨te
- [ ] DÃ©finir la fonction de loss (CrossEntropyLoss)
- [ ] Configurer l'optimiseur (Adam recommandÃ©)
- [ ] Tester sur un petit batch

#### ğŸ’¡ Pipeline complet
```
Image â†’ Encoder â†’ Feature vector â†’ Decoder â†’ Caption
                                      â†‘
                                 Previous words
```

---

### Phase 6 : EntraÃ®nement

#### âœ… TÃ¢ches Ã  rÃ©aliser
- [ ] ImplÃ©menter la boucle d'entraÃ®nement
- [ ] Ajouter la validation aprÃ¨s chaque epoch
- [ ] ImplÃ©menter le teacher forcing
- [ ] Sauvegarder les checkpoints
- [ ] Logger les mÃ©triques (loss, perplexity)
- [ ] Visualiser les courbes d'apprentissage
- [ ] Ajuster les hyperparamÃ¨tres

#### âš™ï¸ HyperparamÃ¨tres Ã  tester
- Learning rate : 0.001, 0.0001
- Batch size : 32, 64, 128
- Hidden size LSTM : 256, 512
- Embedding dimension : 256, 512
- Nombre d'epochs : 20-50

---

### Phase 7 : Ã‰valuation et AmÃ©lioration

#### âœ… TÃ¢ches Ã  rÃ©aliser
- [ ] ImplÃ©menter la gÃ©nÃ©ration de captions (beam search ou greedy)
- [ ] Calculer les mÃ©triques BLEU
- [ ] Analyser les rÃ©sultats qualitatifs
- [ ] Identifier les cas d'Ã©chec
- [ ] AmÃ©liorer le modÃ¨le (data augmentation, dropout, etc.)

#### ğŸ“Š MÃ©triques d'Ã©valuation
- BLEU-1, BLEU-2, BLEU-3, BLEU-4
- Analyse visuelle des captions gÃ©nÃ©rÃ©es

---

### Phase 8 : PrÃ©paration de la DÃ©mo
#### âœ… TÃ¢ches Ã  rÃ©aliser
- [ ] CrÃ©er un script de dÃ©mo simple
- [ ] Tester avec plusieurs images
- [ ] PrÃ©parer une interface de visualisation
- [ ] Optimiser le temps d'infÃ©rence
- [ ] PrÃ©parer des exemples de succÃ¨s et d'Ã©checs
- [ ] Documenter les choix techniques

#### ğŸ¬ Format de la dÃ©mo
```python
# demo.py
1. Charger le modÃ¨le entraÃ®nÃ©
2. Charger l'image fournie
3. GÃ©nÃ©rer la caption
4. Afficher image + caption
```

---

### Phase 9 : Finalisation

#### âœ… TÃ¢ches Ã  rÃ©aliser
- [ ] Nettoyer le code
- [ ] Ajouter des commentaires
- [ ] Finaliser le README
- [ ] PrÃ©parer les rÃ©ponses aux questions potentielles
- [ ] RÃ©pÃ©ter la prÃ©sentation
- [ ] VÃ©rifier que tout fonctionne

#### ğŸ¯ Points clÃ©s pour l'examen
- Comprendre chaque composant du modÃ¨le
- Savoir expliquer les choix d'architecture
- ÃŠtre capable de discuter des rÃ©sultats
- ConnaÃ®tre les limites du modÃ¨le

---

## ğŸš€ Installation et Utilisation

### Installation

```bash
# Cloner le repository
git clone <votre-repo>
cd image-captioning

# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### EntraÃ®nement

```bash
python train.py --data_path ./data/flickr8k \
                --epochs 30 \
                --batch_size 64 \
                --lr 0.001
```

### Ã‰valuation

```bash
python evaluate.py --model_path ./checkpoints/best_model.pth \
                   --image_path ./test_images/
```

### DÃ©mo

```bash
python demo.py --model_path ./checkpoints/best_model.pth \
               --image_path ./exam_image.jpg
```

## ğŸ“ˆ RÃ©sultats Attendus

- **Loss** : Doit diminuer progressivement
- **BLEU-4** : > 0.15-0.20 pour un modÃ¨le from scratch sur Flickr8k
- **QualitÃ© visuelle** : Captions cohÃ©rentes pour des images simples

## ğŸ¤ Travail en Ã‰quipe

### RÃ©partition suggÃ©rÃ©e des tÃ¢ches

**Membre 1** :
- PrÃ©traitement des donnÃ©es
- ImplÃ©mentation de l'encoder
- EntraÃ®nement du modÃ¨le

**Membre 2** :
- Construction du vocabulaire
- ImplÃ©mentation du decoder
- Ã‰valuation et dÃ©mo

**Ensemble** :
- Architecture globale
- Debugging
- PrÃ©paration de la prÃ©sentation

## ğŸ“ Questions Potentielles pour l'Examen

1. **Architecture**
   - Pourquoi utiliser un CNN pour l'encoder ?
   - Pourquoi un LSTM pour le decoder ?
   - Qu'est-ce que le teacher forcing ?

2. **EntraÃ®nement**
   - Quelle fonction de loss avez-vous utilisÃ©e ?
   - Comment gÃ©rez-vous les sÃ©quences de longueur variable ?
   - Quels sont vos hyperparamÃ¨tres ?

3. **RÃ©sultats**
   - Quelles sont les performances de votre modÃ¨le ?
   - Quelles sont les limites ?
   - Comment pourriez-vous l'amÃ©liorer ?

## ğŸ”§ Conseils Pratiques

1. **Commencez simple** : Testez sur un petit subset avant l'entraÃ®nement complet
2. **Sauvegardez rÃ©guliÃ¨rement** : Checkpoints aprÃ¨s chaque epoch
3. **Visualisez** : Regardez des exemples de captions pendant l'entraÃ®nement
4. **DÃ©buggez progressivement** : Testez chaque composant sÃ©parÃ©ment
5. **Documentez** : Notez tous vos choix et expÃ©rimentations

## ğŸ“š Ressources SupplÃ©mentaires

- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [Show and Tell Paper](https://arxiv.org/abs/1411.4555)
- [COCO Dataset](https://cocodataset.org/)
- [BLEU Score Explanation](https://en.wikipedia.org/wiki/BLEU)

## ğŸ“„ Licence

Ce projet est rÃ©alisÃ© dans le cadre d'un cours de Deep Learning.

## ğŸ‘¥ Auteurs

- [Votre Nom]
- [Nom de votre coÃ©quipier]

---

**Bon courage pour votre projet ! ğŸ“**